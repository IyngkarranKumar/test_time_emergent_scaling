{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "import conf, utils, analysis_utils\n",
    "importlib.reload(conf)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from conf import config\n",
    "from utils import *\n",
    "from analysis_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify\n",
    "\n",
    "# Parse the gold and answer\n",
    "# If you know that gold will only contain latex or expr (no latex env), use\n",
    "# parse(gold, extraction_config=[LatexExtractionConfig()]) or parse(gold, extraction_config=[ExprExtractionConfig()])\n",
    "t1=\"72\"\n",
    "t2=\"72<im_end>\"\n",
    "\n",
    "gold = parse(t1)\n",
    "answer = parse(t2)\n",
    "\n",
    "score=verify(gold, answer)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_disk_dir=os.environ.get(\"scratch_disk_dir\")\n",
    "data_dir=\"data/DeepSeek-R1-Distill-Qwen-1.5B_gsm8k_06-15_16-52-17\"\n",
    "\n",
    "config,SAVE_DATA=read_save_data(data_dir)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty dataframe with token budgets as columns and sample indices as rows\n",
    "token_budgets = sorted(list(SAVE_DATA.keys()))\n",
    "n_samples = len(SAVE_DATA[token_budgets[0]].keys()) - 1  # -1 to exclude 'metrics' key\n",
    "df = pd.DataFrame(index=range(n_samples), columns=token_budgets)\n",
    "\n",
    "# Fill dataframe with text data\n",
    "for token_budget in token_budgets:\n",
    "    for sample_idx in range(n_samples):\n",
    "        df.loc[sample_idx, token_budget] = SAVE_DATA[token_budget][sample_idx]['text']\n",
    "\n",
    "with open('data.html', 'w') as f:\n",
    "    f.write(df.to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", cache_dir=os.environ.get(\"HF_CACHE_PATH\"))\n",
    "\n",
    "# Print chat templates for various models\n",
    "models = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    \"google/gemma-7b-it\",\n",
    "    \"deepseek-ai/deepseek-coder-7b-instruct\",\n",
    "    \"Qwen/Qwen1.5-7B-Chat\",\n",
    "    \"microsoft/phi-2\",\n",
    "    \"simplescaling/s1.1-32B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=os.environ.get(\"HF_CACHE_PATH\"))\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "        print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "        print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "        print(f\"BOS token: {tokenizer.bos_token}\")\n",
    "    except:\n",
    "        print(f\"Error loading tokenizer for {model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis_utils\n",
    "importlib.reload(analysis_utils)\n",
    "from analysis_utils import *\n",
    "\n",
    "\n",
    "path=\"data/quick_save\"\n",
    "config,SAVE_DATA=read_save_data(path)\n",
    "\n",
    "completions_text_view(SAVE_DATA,token_budget=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", cache_dir=os.environ.get(\"HF_CACHE_PATH\"))\n",
    "\n",
    "def no_right_or_middle_padding(input_ids, tokenizer):\n",
    "    # Convert to tensor if not already\n",
    "    if not isinstance(input_ids, torch.Tensor):\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "    \n",
    "    # Get positions of all pad tokens\n",
    "    pad_positions = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "    \n",
    "    if len(pad_positions) == 0:\n",
    "        return True\n",
    "        \n",
    "    # Get positions of all non-pad tokens\n",
    "    nonpad_positions = (input_ids != tokenizer.pad_token_id).nonzero()\n",
    "    \n",
    "    # Check that all pad tokens come before all non-pad tokens\n",
    "    return all(p < nonpad_positions[0].item() for p in pad_positions)\n",
    "\n",
    "\n",
    "def no_right_padding(input_ids, tokenizer):\n",
    "    # Convert to tensor if not already\n",
    "    if not isinstance(input_ids, torch.Tensor):\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "    \n",
    "    # Get positions of all pad tokens\n",
    "    pad_positions = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "    \n",
    "    if len(pad_positions) == 0:\n",
    "        return True\n",
    "        \n",
    "    # Get positions of all non-pad tokens\n",
    "    nonpad_positions = (input_ids != tokenizer.pad_token_id).nonzero()\n",
    "    \n",
    "    # Check that all pad tokens come after all non-pad tokens\n",
    "    return all(p > nonpad_positions[-1].item() for p in pad_positions)\n",
    "\n",
    "# Test cases\n",
    "test_seqs = [\n",
    "    torch.tensor([1, 2, 3, 4, 5]),  # No padding\n",
    "    torch.tensor([1, 2, tokenizer.pad_token_id, tokenizer.pad_token_id]),  # Right padding\n",
    "    torch.tensor([tokenizer.pad_token_id, 1, 2, 3]),  # Left padding\n",
    "    torch.tensor([1, tokenizer.pad_token_id, 2, 3])  # Middle padding\n",
    "]\n",
    "\n",
    "for i, seq in enumerate(test_seqs):\n",
    "    print(f\"Sequence {i}: {no_right_or_middle_padding(seq, tokenizer)}\")\n",
    "    print(f\"Sequence {i}: {no_right_padding(seq, tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_names = [\n",
    "    #\"EleutherAI/pythia-70m\",\n",
    "    #\"Qwen/QwQ-32B\", \n",
    "    \"google/gemma-2-2b-it\",\n",
    "\n",
    "    #\"open-thoughts/OpenThinker-32B\"\n",
    "]\n",
    "\n",
    "for name in tokenizer_names:\n",
    "    print(f\"\\n--- Inspecting tokenizer: {name} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "    chat_template = getattr(tokenizer, \"chat_template\", None)\n",
    "    if chat_template is not None:\n",
    "        print(f\"Chat template found for {name}.\")\n",
    "    else:\n",
    "        print(f\"No chat template found for {name}.\")\n",
    "\n",
    "    print(f\"BOS token: {tokenizer.bos_token}\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token}\") \n",
    "    print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "\n",
    "    # Example chat messages (if chat template exists)\n",
    "    if chat_template is not None:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"I'm good, thank you! How can I help you today?\"},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        print(f\"\\nChat prompt: {prompt}\")\n",
    "        tokens = tokenizer(prompt)\n",
    "        print(f\"Tokenized chat prompt: {tokens['input_ids']}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(tokens['input_ids'])}\")\n",
    "\n",
    "        print('\\n\\n\\n')\n",
    "    else:\n",
    "        # If no chat template, just tokenize as normal\n",
    "        prompt = \"Hello, how are you?\"\n",
    "        tokens = tokenizer(prompt)\n",
    "        print(f\"Tokenized prompt: {tokens['input_ids']}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(tokens['input_ids'])}\")\n",
    "        print('\\n\\n\\n')\n",
    "    \n",
    "\n",
    "   \n",
    "#find end of input ids \n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "for toks,ids in list(zip(tokens,token_ids)):\n",
    "    print(f\"{toks}: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"prompts/trace_decomposition.txt\"\n",
    "with open(path, \"r\") as f: \n",
    "    output = f.read()\n",
    "\n",
    "splitter_string = \"\\n\\n\\n\\n\"\n",
    "\n",
    "input_trace = \"SAMPLE_INPUT_TRACE\"\n",
    "input_trace_identifier = \"INSERT_REASONING_TRACE\"\n",
    "input_prompt = \"SAMPLE_INPUT_PROMPT\"\n",
    "input_prompt_identifier = \"INSERT_INPUT_PROMPT\"\n",
    "\n",
    "SYSTEM_PROMPT = output.split(\"SYSTEM_PROMPT=\")[1].split(\"\\n\")[0].strip()\n",
    "USER_PROMPT = output.split(\"USER_PROMPT\")[1].split(\"=\")[-1].strip()\n",
    "\n",
    "\n",
    "USER_PROMPT = USER_PROMPT.replace(input_trace_identifier, input_trace)\n",
    "USER_PROMPT = USER_PROMPT.replace(input_prompt_identifier, input_prompt)\n",
    "\n",
    "print(SYSTEM_PROMPT)\n",
    "print(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create dummy data: 3 rows, 4 columns, each cell is a tuple (a, b)\n",
    "data = [\n",
    "    [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')],\n",
    "    [(5, 'e'), (6, 'f'), (7, 'g'), (8, 'h')],\n",
    "    [(9, 'i'), (10, 'j'), (11, 'k'), (12, 'l')]\n",
    "]\n",
    "\n",
    "columns = ['Col1', 'Col2', 'Col3', 'Col4']\n",
    "index = ['Row1', 'Row2', 'Row3']\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns, index=index)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pythia_tokenizer=AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\", cache_dir=os.environ.get(\"HF_CACHE_PATH\"))\n",
    "\n",
    "# Add special tokens\n",
    "pythia_tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': ['<WACKY1>', '<WACKY2>', '<SUPER_WACKY>']\n",
    "})\n",
    "\n",
    "# Create temp dir and save\n",
    "tmp_dir = \"tmp_tokenizer\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "pythia_tokenizer.save_pretrained(tmp_dir)\n",
    "\n",
    "\n",
    "try:\n",
    "    vllm_model = LLM(model=\"EleutherAI/pythia-70m\",\n",
    "    tokenizer = \"tmp_tokenizer\",\n",
    "    trust_remote_code=True)\n",
    "    print(\"VLLM available\")\n",
    "except Exception as e:\n",
    "    print(f\"VLLM not available: {e}\")\n",
    "\n",
    "# Basic VLLM generation example\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Explain quantum computing in one sentence.\"\n",
    "]\n",
    "\n",
    "outputs = vllm_model.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import sys \n",
    "\n",
    "\n",
    "\n",
    "#clean wipe\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "if \"utils.dataset_utils\" in sys.modules:\n",
    "    del sys.modules[\"utils.dataset_utils\"]\n",
    "\n",
    "\n",
    "\n",
    "import utils\n",
    "import utils.dataset_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[\"math-ai/aime25\",\"Maxwell-Jia/AIME_2024\",\"Idavidrein/gpqa\"]\n",
    "n_samples=10\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_setup = utils.DatasetSetup(dataset_name,n_samples=n_samples)\n",
    "    dataset = dataset_setup.load_standardized_dataset()\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(f\"Number of samples: {len(dataset)}\")\n",
    "    print(\"\\nSample questions and answers:\")\n",
    "    for i, sample in enumerate(dataset):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Question: {sample['question']}\")\n",
    "        print(f\"Answer: {sample['answer']} (type: {type(sample['answer'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer_list = [\n",
    "    #\"EleutherAI/pythia-70m\",\n",
    "    #\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"simplescaling/s1.1-32B\",\n",
    "\n",
    "]\n",
    "\n",
    "if 0: \n",
    "    for tokenizer in tokenizer_list:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        print(f\"\\nTokenizer: {tokenizer.name_or_path}\")\n",
    "        print(\"Special tokens:\")\n",
    "        for token_name, token_value in tokenizer.special_tokens_map.items():\n",
    "            print(f\"{token_name}: {token_value}\")\n",
    "        print(\"---\")\n",
    "\n",
    "def get_end_of_input_idxs(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Gets 'end of input' token indices for each sequence in the batch.\n",
    "    Returns tensor of indices where the end_of_input pattern ends.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fix the condition logic - use 'in' properly\n",
    "    if \"Qwen2.5\" in tokenizer.name_or_path or \"simplescaling\" in tokenizer.name_or_path:\n",
    "        end_of_input_ids = torch.tensor([151644, 77091, 198])  # '<|im_start|>assistant\\n'\n",
    "    elif \"gemma-2\" in tokenizer.name_or_path:\n",
    "        end_of_input_ids = torch.tensor([106, 2516, 108])  # '<start_of_turn>model\\n'\n",
    "    else:\n",
    "        raise ValueError(f\"Tokenizer {tokenizer.name_or_path} not supported for force continuation\")\n",
    "    \n",
    "    end_of_input_idxs = []\n",
    "    pattern_len = len(end_of_input_ids)\n",
    "    \n",
    "    for seq in input_ids:\n",
    "        found = False\n",
    "        # Use sliding window to find pattern\n",
    "        for i in range(len(seq) - pattern_len + 1):\n",
    "            if torch.all(seq[i:i+pattern_len] == end_of_input_ids):\n",
    "                # Return the index AFTER the pattern (where generation starts)\n",
    "                end_of_input_idxs.append(list(range(i,i+pattern_len)))\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            # If pattern not found, assume entire sequence is input\n",
    "            end_of_input_idxs.append(len(seq))\n",
    "            \n",
    "    return end_of_input_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample input sentences\n",
    "input_sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "]\n",
    "\n",
    "# Apply chat template and compare for each tokenizer\n",
    "for tokenizer_name in tokenizer_list:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    print(f\"\\nTokenizer: {tokenizer.name_or_path}\")\n",
    "    \n",
    "    for sentence in input_sentences:\n",
    "        print(f\"\\nOriginal input: {sentence}\")\n",
    "        \n",
    "        # Get base tokenization\n",
    "        base_tokens = tokenizer.encode(sentence)\n",
    "        print(f\"Base tokenization: {base_tokens}\")\n",
    "        print(f\"Decoded base tokens: {tokenizer.decode(base_tokens)}\")\n",
    "        \n",
    "        # Apply chat template if available\n",
    "        if tokenizer.chat_template is not None:\n",
    "            formatted = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sentence}], tokenize=False,add_generation_prompt=True)\n",
    "            formatted_tokens = tokenizer.encode(formatted)\n",
    "            print(f\"Chat template: {formatted}\")\n",
    "            print(f\"Chat template tokens: {formatted_tokens}\")\n",
    "            print(f\"Decoded chat template: {tokenizer.decode(formatted_tokens)}\")\n",
    "            \n",
    "            # Get end of input indices\n",
    "            eoi_idxs = get_end_of_input_idxs(torch.tensor([formatted_tokens]), tokenizer)\n",
    "            print(f\"End of input index: {eoi_idxs}\")\n",
    "            print(f\"End of input token: {formatted_tokens[eoi_idxs]}\")\n",
    "            print(f\"Decoded end of input: {tokenizer.decode(formatted_tokens[eoi_idxs])}\")\n",
    "        else:\n",
    "            print(\"No chat template available\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chat templates for each tokenizer\n",
    "for tokenizer_name in tokenizer_list:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    print(f\"\\nTokenizer: {tokenizer.name_or_path}\")\n",
    "    print(f\"Chat template: {tokenizer.chat_template}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import random\n",
    "\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.analysis_utils)\n",
    "from utils.analysis_utils import *\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = generate_dummy_data([128, 256, 512, 1024, 2048], 5, 3)\n",
    "\n",
    "metrics=[\"score\",\"probs\",\"entropy\"]\n",
    "utils.plot_data(n_samples=2,SAVE_DATA=dummy_data,metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "n_ys=12\n",
    "x = np.linspace(0, 10, 100)\n",
    "ys = []\n",
    "labels = []\n",
    "\n",
    "# Generate n_ys different curves\n",
    "for i in range(n_ys):\n",
    "    curve_type = i % 3  # Cycle through 3 different curve types\n",
    "    if curve_type == 0:\n",
    "        y = np.sin(x + i/2) + 0.1*np.random.randn(len(x))\n",
    "        labels.append(f'sin(x+{i/2:.1f})')\n",
    "    elif curve_type == 1:\n",
    "        y = ((x+i)**2)/100 + 0.1*np.random.randn(len(x))\n",
    "        labels.append(f'(x+{i})^2')\n",
    "    else:\n",
    "        y = np.exp(-(x+i)/5) + 0.1*np.random.randn(len(x))\n",
    "        labels.append(f'exp(-(x+{i})/5)')\n",
    "    ys.append(y)\n",
    "\n",
    "# Plot curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "for y, label in zip(ys, labels):\n",
    "    plt.plot(x, y, label=label)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot distribution of differences for all curves\n",
    "plt.subplot(122)\n",
    "for y, label in zip(ys, labels):\n",
    "    plt.hist(np.diff(y), bins=20, density=True, alpha=0.5, label=label)\n",
    "plt.xlabel('Difference')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Differences')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ys=np.array(ys)\n",
    "diffs=np.diff(ys,axis=1) \n",
    "n_clusters=3\n",
    "\n",
    "# Reshape diffs to 2D array for clustering\n",
    "diffs_reshaped = diffs.reshape(diffs.shape[0], -1)\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(diffs_reshaped)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# Plot original curves colored by cluster\n",
    "plt.subplot(121)\n",
    "for i, (y, label) in enumerate(zip(ys, labels)):\n",
    "    plt.plot(x, y, label=f'Cluster {clusters[i]}', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Curves Colored by Difference Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot cluster centers\n",
    "plt.subplot(122)\n",
    "centers = kmeans.cluster_centers_.reshape(n_clusters, -1)\n",
    "for i, center in enumerate(centers):\n",
    "    plt.plot(np.arange(len(center)), center, label=f'Cluster {i} Center')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Difference')\n",
    "plt.title('Cluster Centers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Force set the environment variable\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE' #fix this bug\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import conf\n",
    "\n",
    "importlib.reload(conf)\n",
    "importlib.reload(utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/iyngkarrankumar/Documents/Edinburgh MScR/Easter (Q3)/random_experiments/budget_forcing_emergence/results_data/DeepSeek-R1-Distill-Qwen-1.5B_gsm8k_07-05_19-55-37\"\n",
    "\n",
    "config,SAVE_DATA=utils.read_save_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.token_budget_text_view(SAVE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gdrive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import utils, config, utils.analysis_utils as analysis_utils\n",
    "\n",
    "path = \"/Users/iyngkarrankumar/Google Drive/My Drive/results_data\"\n",
    "\n",
    "config,SAVE_DATA = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_names = [\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "]\n",
    "\n",
    "test_text = \"Quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "        if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "            print(\"Chat template found.\")\n",
    "            try:\n",
    "                chat_encoded = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": test_text}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True if \"llama\" in model_name.lower() or \"qwen\" in model_name.lower() else False\n",
    "                )\n",
    "                print(\"Chat template output:\", chat_encoded)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying chat template: {e}\")\n",
    "        else:\n",
    "            print(\"No chat template found.\")\n",
    "        print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load tokenizer for {model_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import importlib\n",
    "import utils\n",
    "import utils\n",
    "\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.analysis_utils)\n",
    "all_paths_dir = \"results_data/multi_run_results\"\n",
    "\n",
    "paths = os.listdir(all_paths_dir)\n",
    "paths = [os.path.join(all_paths_dir, path) for path in paths]\n",
    "save_dir = \"analysis_outputs\"\n",
    "analysis_workflow = utils.analysis_utils.AnalysisWorkflow(paths=paths, results_save_dir=save_dir)\n",
    "\n",
    "#analysis_workflow.aggregate_plots(SHOW=False)]\n",
    "\n",
    "model_name = \"deepseek\"\n",
    "dataset_name = \"aime_2024\"\n",
    "tables = analysis_workflow.get_metric_tables(model_name=model_name,dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "x = np.array([0.        , 0.        , 0.        , 0.01333333, 0.2       ])\n",
    "y = np.array([0.00163385, 0.00186161, 0.00218855, 0.01744281, 0.19760427])\n",
    "\n",
    "mu_x = np.mean(x)\n",
    "mu_y = np.mean(y)\n",
    "\n",
    "x_mu_dist = x-mu_x\n",
    "y_mu_dist = y-mu_y\n",
    "x_mu_dist_sq = x_mu_dist**2\n",
    "y_mu_dist_sq = y_mu_dist**2\n",
    "\n",
    "mu_dist_product = x_mu_dist * y_mu_dist\n",
    "\n",
    "\n",
    "numerator = np.sum(mu_dist_product)\n",
    "denominator = np.sqrt(np.sum(x_mu_dist_sq) * np.sum(y_mu_dist_sq))\n",
    "\n",
    "pearson_corr = numerator/denominator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
