
#data 
dataset_name: 
  - "math-ai/aime25"
  - "Maxwell-Jia/AIME_2024"
  - "Idavidrein/gpqa"


#30 samples 
#5 samples per batch 
#effective batch size = 25
num_samples: 30
batch_size: 5
num_completions: 5


#model
model_name: "Qwen/Qwen2.5-14B-Instruct"
quantization: 8

#workflow 
start_token_budget: 9 #log2
end_token_budget: 14 #log2

#scoring
normalise_over_solution_set: True
solution_set_batch_size: 20 

#SAVE
SAVE_BOOL: True

#inference
inference_engine: "vllm"
vllm_gpu_memory_utilization: 0.6

